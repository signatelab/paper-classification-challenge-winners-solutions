# 【SIGNATE】医学論文自動仕分けコンペ解法

[SIGNATE医学論文自動仕分けコンテスト](https://signate.jp/competitions/471)で、Public：22位，Private：2位となり最終評価2位で入賞候補として選ばれたため、解法の提出を行います。手法的に複雑なことをやっているわけではないと思っているので、もしかすると逆に伸びしろはあるのかもしれません。他の方々の解法と比較して頂きたく思います。

また、当方コンペ経験浅く、5サブ以上行ったものは2コンペのみです。不足点あった際にはお詫び致します。

# コンテストの概要

論文のタイトルおよび抄録のテキストデータを用いて、システマティックレビューの対象となる文献か否か（2値）を判定するアルゴリズムを作成するコンテスト。

システマティック・レビューとは、ライフサイエンス、特に医学分野において浸透している研究方法で、特定の研究テーマに対する文献をくまなく調査し、各研究データのバイアスを評価しながら、体系的に同質の研究データを収集・解析する研究手法のことを指す。本コンテストではこの作業の省略化を目指して、収集された論文の中から目的の論文を「選別」するための機械学習アルゴリズムの構築を行う。

引用：https://signate.jp/competitions/471

# 環境
* マシン：Google Colab Pro+
    * NVIDIA Tesla P100
* OS：Ubuntu 18.04.5 LTS (Bionic Beaver)
* 言語：Python3.7.12
* ライブラリ：[requirements.txt](https://drive.google.com/file/d/1pxEX5DwR1c3lYI7qmj8IipgNf9RsaWP6/view?usp=sharing)

# 方針
以下に取り組み方針を示す。まず、本コンペのフォーラムでYoshio Sugiyamaさんが公開して下さった[ベースライン](https://signate.jp/competitions/471/discussions/pytorch-bert)を参考にしており、コードの流れは基本的に同じです。

## 採用したモデリング手法の概要

### 特徴量
titleとabstractを結合したテキストを入力データとする

### モデル
BERT系の事前学習済みモデルである`microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract`を利用し、層化K分割交差検証(K=10)により、10個のモデルで学習、最終的な予測値をもとめた。

また、この事前学習済みモデルは[こちら](https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract)のもので、以下4点を満たす。
a. ソースが明確である
b. 誰でも利用することができる
c. 無償で利用することができる
d. オープンソースとして再配布することができる

## 工夫点・モデリングの特徴
### 工夫点
基本方針はBERTを用いた層化K分割交差検証(K=10)による学習と予測のみです。
主な工夫点は以下の3つのみです。

* PubMedBERTの利用
    * PubMedで公開されている生物系の論文のabstractを用いて事前学習されている。
    * フォーラムで[Discussion1](https://signate.jp/competitions/471/discussions/pretraining-model)や[Discussion2](https://signate.jp/competitions/471/discussions/20210817130616-69385)で言及されていたが、本コンペでとても効果を発揮する事前学習済みモデルと考えられる。
    * Fold5,epoch5でPubMedBERTに変えたとたんにPublicLBで0.920程度出て、一気に上位に食い込んだ。
* Fold数を10とする
    * 期間の2/3が経過したあたりで、アンサンブルを考えたが、他のBERT系のモデルとアンサンブルすると、直感的にスコアが悪くなる気がしたので、Fold数を上げ、1個のモデル当たりの訓練データを増やすことを考えた。
    * 極端な話今回のようなラベル数に極度の偏りのある場合は、leave-one-outに近いようなモデルの作成をするとデータを有効活用できるではと考えた。
    * 実際、閾値を変えても、Fold5よりもFold10の方がPublic・Privateともに高いスコアがでた。
* 閾値の調整
    * [Discussion](https://signate.jp/competitions/471/discussions/20210811010017-65009)でも言及されている通り、本コンペでは閾値の調整が大事になっている。そのため、訓練データとテストデータの分布が同じだと仮定して、oof値から最もスコアが高くなる閾値を探索したり、或いは分布がやや異なると仮定して、PublicLBのスコアを見ながら閾値を調整したりなど行いました。
    * Fold10・閾値の調整合わせて、0.9219程度出て、最終的にPublicLB:22位でしたが、PrivateLB:2位で大幅にshake upしました(~~完全に運~~)。


### 他取り組んだこと
* BERT・RoBERTa・SciBERT・BlueBERT
    * 手法によって変わるかと思いますが、精度面で見ると最もシンプルな方法でBERT < RoBERTa < BlueBERT = SciBERT < PubMedBERTの印象でした。
* epochs・max_length・Foldの変更
    * epochs:3よりも5が良かったです。3の時点でBestScoreになるので5までやる必要はないのですが、なぜか5の方が良かったので、ずっと5でやっていました(乱数の問題かと)。
    * max_length:モデルの統一目的と、出来るだけ長くとった方が良いとの考えから、最初から500~512で行っていました。
* 記号・stopwordsの削除、大文字を小文字に統一
    * 色々な組み合わせで行いましたが、どの組み合わせでも処理なしの場合よりもPublicLBの値が下がったので取り入れませんでした。
* PubMedBERTのEmbeddingsをConv1Dで2回畳み込みを行うことでFine-tuning
    * [こちら](https://www.ai-shift.co.jp/techblog/2145)を読んで、良さそうだと思ったので試しました。
    * リソースと時間の関係でkenel_size=2しか試せませんでしたが、通常のCLSをLinearに通すよりも高くはなりませんでした。
* データの水増し(逆翻訳)
    * 本コンペには合わなそうだと思いつつ、正例について水増ししてみました。
    * CVについては似たデータが2つ存在することになり、実際よりも高くなってしまいましたが、PublicLBでは下がることはあっても上がることはありませんでした。
* scipy.optimizeによる閾値の最適化
    * フォーラムでどなたかが言及してたものですが、閾値のあたりをつけるのに使用しました。
    * 最終的にはLBを見ながら、ちびちび調整することになりました。

## 分析・モデリングから得られた示唆

### どんな特徴のあるデータだったか
* 正例と負例でかなり差があるデータ
    * モデル作成の際には出来るだけ、訓練データ中の1の割合を多くしたい
* 正例と負例の場合で出現する単語に偏りがありそう
    * [Discussion](https://signate.jp/competitions/471/discussions/20210807122917-65009)を読み分かったが、直接モデリングに組み込んではいない。

### どういう点が難しい課題だったか
* 自然言語処理系コンペ初参加なので間違っていたら申し訳ないが、今回のデータは長めのテキストデータもある。最長1000程度のデータもあった。また、語彙数もそれなりに多いと考えられるので、外部のデータを全く使用しない(事前学習済みモデル)で、学習・予測を行うには限界があるのではと感じた。
* データは全て医学系の論文であり、その中でさらに特定の論文を見つけるということで、そもそも出現する単語が似通っていることが考えられる。そのため、実際には文脈や少数の出現単語から判断しなければならないのではと考えた。ただ、仮に少数の出現単語から判断できるのであれば、それを発見さえ出来ればもっと精度はあがることが期待されるが、そこまでは出来なかった。

## モデルを再現するための手順
NoteBookのプログラムは以下のような流れになっている。詳しくはNoteBook参照。

1. 乱数シードの固定
    * pythonの組込み関数・numpyで使われるrandomのseed値を固定.
    * GPU関連のseed値を固定.
1. データ読み込み・前処理
    * データはGoogleドライブ上に置いた。
    * 訓練データのうち、id=2488とid=7708はjudgement=0に置き換える
    * 訓練データは交差検証用に訓練・検証データとしてFold10分、分割する
    * テストデータは特に何もしない
1. データセット定義
    * データの読み込み・加工と文章のID化を行い、BERTの読み込める形式に変換する
1. モデル定義
    * BERTモデルの定義
1. 実行時間計測用ツール・学習補助関数・検証補助関数の定義
    * 学習補助関数：1epoch内でiterごとにパラメータの更新を行う関数。
    * 検証補助関数：予測を行う関数。
1. 推論関数の定義
    * 保存されている学習済みモデルによって、テストデータに対する予測を行う関数
1. 学習用関数の定義
    * epoch5で、Foldごとに扱うデータを変更させて学習を行う関数
1. メイン
    * 学習
    * 予測

最後に、最終投稿ファイルのプログラムからの再現はおそらく困難です。

ここで理由を述べさせて頂くと、実はコンペ中に投稿したファイルのモデルを作成する際に、途中でマシン(Google Colab)が落ちて、実行が中断されていました。しかし、途中の結果が保存されていたので、途中から学習を再開し、それらのモデルを使用してsubmitを行いました。

しかし、今回再現性を確かめるために、中断がないように学習を実行したところ、最終的な予測値に差が出てしまいました。具体的に言うと、交差検証Fold10で学習を行い、Fold9とFold10で作成したモデルで予測した値に差が出ました(途中で中断された部分)。

GPUを使用しており、seed値は固定しているのですが、おそらく、Fold9とFold10で途中から学習を行うのと、最初から一気通貫で学習を行うのとで、結果の差につながるのだと考えています(実際にFold8までは同一のモデルが作成できている)。


上記のことはお問い合わせからもご報告させて頂いており、以下のようにご指示頂いたため、該当ファイル群を提出させて頂いております。

>
今回、実行途中で中断されて投稿時のモデルがお手元で厳密に再現できないとのことでしたが、
・投稿時に使用した学習済みモデル
・提出用のソースコードで（中断なく実行した結果）生成される学習済みモデル
の両方をご提出頂ければ、
まず前者の投稿時のモデルでスコアを確認したのち、
後者のソースコードから生成されるモデルとのスコア差も小さいようであれば、
問題なく再現性が担保されたものとして判断させて頂きます


## 学習部分のおおよその実行時間を示してください
先述した環境での実行時間です。

* 1Fold：1300s程度/epoch×5epoch
* 10Fold合計：18時間程度(学習部分の実行時間)

## 予測部分のおおよその実行時間を示してください
* 1Fold：12分
* 10Fold合計：2時間程度(予測部分の実行時間)